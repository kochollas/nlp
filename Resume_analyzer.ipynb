{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode LOC\n",
      "earlier this week DATE\n",
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n"
     ]
    }
   ],
   "source": [
    "text = (\"Hello everyone i've some good news to give you\")\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "cleaned = [ y for y in doc if not y.is_stop and y.pos_  != 'PUNCT']\n",
    "cleaned\n",
    "\n",
    "#raw = [(x.lemma_, x.pos_) for x in cleaned]\n",
    "#raw = [x.lemma_ for x in doc]\n",
    "#print(raw)\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "    \n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "#print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sebastian Thrun', 'PERSON'), ('Google', 'ORG'), ('2007', 'DATE'), ('American', 'NORP'), ('Thrun', 'PERSON'), ('Recode', 'LOC'), ('earlier this week', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "ent = [(x.text, x.label_) for x in doc.ents]\n",
    "print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/mikes/Documents/jobs2020/Resumes/CV03.pdf',\n",
       " '/home/mikes/Documents/jobs2020/Resumes/Resume0221.pdf',\n",
       " '/home/mikes/Documents/jobs2020/Resumes/Resume.pdf',\n",
       " '/home/mikes/Documents/jobs2020/Resumes/main.pdf',\n",
       " '/home/mikes/Documents/jobs2020/Resumes/CV02.pdf',\n",
       " '/home/mikes/Documents/jobs2020/Resumes/Resume01.pdf',\n",
       " '/home/mikes/Documents/jobs2020/Resumes/CV01.pdf']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Function to read resumes from the folder one by one\n",
    "mypath='/home/mikes/Documents/jobs2020/Resumes' #enter your path here where you saved the resumes\n",
    "onlyfiles = [os.path.join(mypath, f) for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n",
    "\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICHAELOCHOLA\n",
      "kochollamikes@gmail.com,0715029616\n",
      "PROFILE\n",
      "DataProfessionalwithstrongtheoreticalskillsinStatisticsandComputing.Ilovetoautomatedataprocessesanddemocratizedata\n",
      "accessibilitywithafocusonqualityandmaintainability.\n",
      "EDUCATION\n",
      "BSc.Mathematics&ComputerScience\n",
      "Comp:2014\n",
      "JKUAT\n",
      "WORKEXPERIENCE\n",
      "DataScienceconsultant\n",
      "Jan\n",
      "2020-Present\n",
      "AfterWorkDataScienceAcademy&DataSeniorsTechnologies\n",
      "DataScientist\n",
      "May2019-\n",
      "Dec2020\n",
      "EnezaEducationLtd\n",
      "\n",
      "DevelopautomatedETLprocessestostreamlineAnalyticsdatademands\n",
      "\n",
      "Design,developanddeploydashboardstomeetEneza'sreportingrequirements\n",
      "\n",
      "Subscriberbaseuserresearchonchurn,acquisitionandretention\n",
      "DataScientist\n",
      "Nov2018-Apr\n",
      "2019\n",
      "Hivisasa.com\n",
      ".\n",
      "\n",
      "Topnotchdatavisualizationonchurnandretentionforwriters\n",
      "\n",
      "Design,developanddeploydataprocessingapplications.\n",
      "ComputerAnalyst\n",
      "Sep2014-Nov\n",
      "2018\n",
      "UniversityofNairobi\n",
      "\n",
      "Designanddevelopwebapplicationstomanagedemographicresearch&surveillance.\n",
      "\n",
      "Buildtoolstoalloweasydataaccessamongacademicandpostgraduatestudents\n",
      "\n",
      "ConsultedasadatamanageronvariousinternationalresearchprojectsbyUONacademic\n",
      "SKILLS\n",
      "Programming:\n",
      "Python,R,PHP,C++\n",
      "DataVisualization:\n",
      "ggplot2,matplotlib,Highcharts,Tableau,Spreadsheets\n",
      "APILibraries:\n",
      "Flask,Plumber\n",
      "MachineLearning:\n",
      "Caret,Scikit-Learn,Statisticalmodeling\n",
      "Deployment:\n",
      "AWSEC2LinuxInstance,HerokuCLI,Shinyapps.io\n",
      "AdditionalLinks\n",
      "\n",
      "Portfolio\n",
      "\n",
      "\n",
      "REFERENCES\n",
      "Availableuponrequest\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"MICHAELOCHOLA\\nkochollamikes@gmail.com,0715029616\\nPROFILE\\nDataProfessionalwithstrongtheoreticalskillsinStatisticsandComputing.Ilovetoautomatedataprocessesanddemocratizedata\\naccessibilitywithafocusonqualityandmaintainability.\\nEDUCATION\\nBSc.Mathematics&ComputerScience\\nComp:2014\\nJKUAT\\nWORKEXPERIENCE\\nDataScienceconsultant\\nJan\\n2020-Present\\nAfterWorkDataScienceAcademy&DataSeniorsTechnologies\\nDataScientist\\nMay2019-\\nDec2020\\nEnezaEducationLtd\\n\\nDevelopautomatedETLprocessestostreamlineAnalyticsdatademands\\n\\nDesign,developanddeploydashboardstomeetEneza'sreportingrequirements\\n\\nSubscriberbaseuserresearchonchurn,acquisitionandretention\\nDataScientist\\nNov2018-Apr\\n2019\\nHivisasa.com\\n.\\n\\nTopnotchdatavisualizationonchurnandretentionforwriters\\n\\nDesign,developanddeploydataprocessingapplications.\\nComputerAnalyst\\nSep2014-Nov\\n2018\\nUniversityofNairobi\\n\\nDesignanddevelopwebapplicationstomanagedemographicresearch&surveillance.\\n\\nBuildtoolstoalloweasydataaccessamongacademicandpostgraduatestudents\\n\\nConsultedasadatamanageronvariousinternationalresearchprojectsbyUONacademic\\nSKILLS\\nProgramming:\\nPython,R,PHP,C++\\nDataVisualization:\\nggplot2,matplotlib,Highcharts,Tableau,Spreadsheets\\nAPILibraries:\\nFlask,Plumber\\nMachineLearning:\\nCaret,Scikit-Learn,Statisticalmodeling\\nDeployment:\\nAWSEC2LinuxInstance,HerokuCLI,Shinyapps.io\\nAdditionalLinks\\n\\nPortfolio\\n\\n\\nREFERENCES\\nAvailableuponrequest\\n\"]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pdfextract(file):\n",
    "    fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
    "    countpage = fileReader.getNumPages()\n",
    "    count = 0\n",
    "    text = []\n",
    "    while count < countpage:    \n",
    "        pageObj = fileReader.getPage(count)\n",
    "        count +=1\n",
    "        t = pageObj.extractText()\n",
    "        print (t)\n",
    "        text.append(t)\n",
    "    return text\n",
    "\n",
    "trial = pdfextract(onlyfiles[3])\n",
    "trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyword_dict = pd.read_csv('/home/mikes/Documents/ML/NLP/template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Machine Learning</th>\n",
       "      <th>Deep Learning</th>\n",
       "      <th>R Language</th>\n",
       "      <th>Python Language</th>\n",
       "      <th>NLP</th>\n",
       "      <th>Data Engineering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>statistical models</td>\n",
       "      <td>linear regression</td>\n",
       "      <td>neural network</td>\n",
       "      <td>R</td>\n",
       "      <td>python</td>\n",
       "      <td>nlp</td>\n",
       "      <td>aws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>probability</td>\n",
       "      <td>logistic regression</td>\n",
       "      <td>keras</td>\n",
       "      <td>ggplot</td>\n",
       "      <td>flask</td>\n",
       "      <td>natural language processing</td>\n",
       "      <td>ec2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal distribution</td>\n",
       "      <td>k means</td>\n",
       "      <td>theano</td>\n",
       "      <td>shiny</td>\n",
       "      <td>django</td>\n",
       "      <td>topic modeling</td>\n",
       "      <td>amazon redshift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poisson distribution</td>\n",
       "      <td>random forest</td>\n",
       "      <td>face detection</td>\n",
       "      <td>cran</td>\n",
       "      <td>pandas</td>\n",
       "      <td>lda</td>\n",
       "      <td>s3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>survival analysis</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>neural networks</td>\n",
       "      <td>dplyr</td>\n",
       "      <td>numpy</td>\n",
       "      <td>named entity recognition</td>\n",
       "      <td>docker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hypothesis testing</td>\n",
       "      <td>svm</td>\n",
       "      <td>Convolutional neural network(CNN)</td>\n",
       "      <td>tidyr</td>\n",
       "      <td>scikitlearn</td>\n",
       "      <td>post tagging</td>\n",
       "      <td>kubernates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bayesian inference</td>\n",
       "      <td>naive bayes</td>\n",
       "      <td>Recurrent neural network(RNN)</td>\n",
       "      <td>lubridate</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>scala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>factor analysis</td>\n",
       "      <td>pca</td>\n",
       "      <td>object detection</td>\n",
       "      <td>knitr</td>\n",
       "      <td>matplotlib</td>\n",
       "      <td>word embedding</td>\n",
       "      <td>teradata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>forecasting</td>\n",
       "      <td>decision trees</td>\n",
       "      <td>yolo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scipy</td>\n",
       "      <td>lsi</td>\n",
       "      <td>google big query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>markov chain</td>\n",
       "      <td>svd</td>\n",
       "      <td>gpu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bokeh</td>\n",
       "      <td>spacy</td>\n",
       "      <td>aws lambda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Statistics     Machine Learning  \\\n",
       "0    statistical models    linear regression   \n",
       "1           probability  logistic regression   \n",
       "2   normal distribution              k means   \n",
       "3  poisson distribution        random forest   \n",
       "4     survival analysis              xgboost   \n",
       "5    hypothesis testing                  svm   \n",
       "6    bayesian inference          naive bayes   \n",
       "7       factor analysis                  pca   \n",
       "8           forecasting       decision trees   \n",
       "9          markov chain                  svd   \n",
       "\n",
       "                       Deep Learning R Language Python Language  \\\n",
       "0                     neural network         R         python     \n",
       "1                              keras     ggplot           flask   \n",
       "2                             theano      shiny          django   \n",
       "3                     face detection       cran          pandas   \n",
       "4                    neural networks      dplyr           numpy   \n",
       "5  Convolutional neural network(CNN)      tidyr     scikitlearn   \n",
       "6      Recurrent neural network(RNN)  lubridate         sklearn   \n",
       "7                   object detection      knitr      matplotlib   \n",
       "8                               yolo        NaN           scipy   \n",
       "9                                gpu        NaN           bokeh   \n",
       "\n",
       "                           NLP  Data Engineering  \n",
       "0                          nlp               aws  \n",
       "1  natural language processing               ec2  \n",
       "2               topic modeling   amazon redshift  \n",
       "3                          lda                s3  \n",
       "4     named entity recognition            docker  \n",
       "5                 post tagging        kubernates  \n",
       "6                     word2vec             scala  \n",
       "7               word embedding          teradata  \n",
       "8                          lsi  google big query  \n",
       "9                        spacy        aws lambda  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_dict.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[linear regression,\n",
       " logistic regression,\n",
       " k means,\n",
       " random forest,\n",
       " xgboost,\n",
       " svm,\n",
       " naive bayes,\n",
       " pca,\n",
       " decision trees,\n",
       " svd,\n",
       " ensemble models,\n",
       " boltzman machine]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_words = [nlp(text) for text in keyword_dict['Machine Learning'].dropna(axis = 0)]\n",
    "ML_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICHAELOCHOLA\n",
      "kochollamikes@gmail.com,0715029616\n",
      "PROFILE\n",
      "DataProfessionalwithstrongtheoreticalskillsinStatisticsandComputing.Ilovetoautomatedataprocessesanddemocratizedata\n",
      "accessibilitywithafocusonqualityandmaintainability.\n",
      "EDUCATION\n",
      "BSc.Mathematics&ComputerScience\n",
      "Comp:2014\n",
      "JKUAT\n",
      "WORKEXPERIENCE\n",
      "DataScienceconsultant\n",
      "Jan\n",
      "2020-Present\n",
      "AfterWorkDataScienceAcademy&DataSeniorsTechnologies\n",
      "DataScientist\n",
      "May2019-\n",
      "Dec2020\n",
      "EnezaEducationLtd\n",
      "\n",
      "DevelopautomatedETLprocessestostreamlineAnalyticsdatademands\n",
      "\n",
      "Design,developanddeploydashboardstomeetEneza'sreportingrequirements\n",
      "\n",
      "Subscriberbaseuserresearchonchurn,acquisitionandretention\n",
      "DataScientist\n",
      "Nov2018-Apr\n",
      "2019\n",
      "Hivisasa.com\n",
      ".\n",
      "\n",
      "Topnotchdatavisualizationonchurnandretentionforwriters\n",
      "\n",
      "Design,developanddeploydataprocessingapplications.\n",
      "ComputerAnalyst\n",
      "Sep2014-Nov\n",
      "2018\n",
      "UniversityofNairobi\n",
      "\n",
      "Designanddevelopwebapplicationstomanagedemographicresearch&surveillance.\n",
      "\n",
      "Buildtoolstoalloweasydataaccessamongacademicandpostgraduatestudents\n",
      "\n",
      "ConsultedasadatamanageronvariousinternationalresearchprojectsbyUONacademic\n",
      "SKILLS\n",
      "Programming:\n",
      "Python,R,PHP,C++\n",
      "DataVisualization:\n",
      "ggplot2,matplotlib,Highcharts,Tableau,Spreadsheets\n",
      "APILibraries:\n",
      "Flask,Plumber\n",
      "MachineLearning:\n",
      "Caret,Scikit-Learn,Statisticalmodeling\n",
      "Deployment:\n",
      "AWSEC2LinuxInstance,HerokuCLI,Shinyapps.io\n",
      "AdditionalLinks\n",
      "\n",
      "Portfolio\n",
      "\n",
      "\n",
      "REFERENCES\n",
      "Availableuponrequest\n",
      "\n",
      "[statistical models, probability, normal distribution, poisson distribution, survival analysis, hypothesis testing, bayesian inference, factor analysis, forecasting, markov chain, monte carlo]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add() takes at most 6 positional arguments (13 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-ef9a292ee059>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Stats'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mstats_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NLP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mNLP_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ML'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mML_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mspacy/matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.Matcher.add (spacy/matcher.cpp:7561)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: add() takes at most 6 positional arguments (13 given)"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "file = onlyfiles[3]\n",
    "text = pdfextract(file) \n",
    "text = str(text)\n",
    "text = text.replace(\"\\\\n\", \"\")\n",
    "text = text.lower()\n",
    "#below is the csv where we have all the keywords, you can customize your own\n",
    "keyword_dict = pd.read_csv('/home/mikes/Documents/ML/NLP/template.csv')\n",
    "stats_words = [nlp(text) for text in keyword_dict['Statistics'].dropna(axis = 0)]\n",
    "NLP_words = [nlp(text) for text in keyword_dict['NLP'].dropna(axis = 0)]\n",
    "ML_words = [nlp(text) for text in keyword_dict['Machine Learning'].dropna(axis = 0)]\n",
    "DL_words = [nlp(text) for text in keyword_dict['Deep Learning'].dropna(axis = 0)]\n",
    "R_words = [nlp(text) for text in keyword_dict['R Language'].dropna(axis = 0)]\n",
    "python_words = [nlp(text) for text in keyword_dict['Python Language'].dropna(axis = 0)]\n",
    "Data_Engineering_words = [nlp(text) for text in keyword_dict['Data Engineering'].dropna(axis = 0)]\n",
    "\n",
    "\n",
    "print(stats_words)\n",
    "\n",
    "#matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('Stats', None, *stats_words)\n",
    "matcher.add('NLP', None, *NLP_words)\n",
    "matcher.add('ML', None, *ML_words)\n",
    "matcher.add('DL', None, *DL_words)\n",
    "matcher.add('R', None, *R_words)\n",
    "matcher.add('Python', None, *python_words)\n",
    "matcher.add('DE', None, *Data_Engineering_words)\n",
    "doc = nlp(text)\n",
    "\n",
    "d = []  \n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "    span = doc[start : end]  # get the matched slice of the doc\n",
    "    d.append((rule_id, span.text))      \n",
    "keywords = \"\\n\".join('{i[0]} {i[1]} ({j})'.format(i,j) for i,j in Counter(d).items())\n",
    "\n",
    "## convertimg string of keywords to dataframe\n",
    "df = pd.read_csv(StringIO(keywords),names = ['Keywords_List'])\n",
    "df1 = pd.DataFrame(df.Keywords_List.str.split(' ',1).tolist(),columns = ['Subject','Keyword'])\n",
    "df2 = pd.DataFrame(df1.Keyword.str.split('(',1).tolist(),columns = ['Keyword', 'Count'])\n",
    "df3 = pd.concat([df1['Subject'],df2['Keyword'], df2['Count']], axis =1) \n",
    "df3['Count'] = df3['Count'].apply(lambda x: x.rstrip(\")\"))\n",
    "\n",
    "base = os.path.basename(file)\n",
    "filename = os.path.splitext(base)[0]\n",
    "\n",
    "name = filename.split('_')\n",
    "name2 = name[0]\n",
    "name2 = name2.lower()\n",
    "## converting str to dataframe\n",
    "name3 = pd.read_csv(StringIO(name2),names = ['Candidate Name'])\n",
    "\n",
    "dataf = pd.concat([name3['Candidate Name'], df3['Subject'], df3['Keyword'], df3['Count']], axis = 1)\n",
    "dataf['Candidate Name'].fillna(dataf['Candidate Name'].iloc[0], inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes at least 2 positional arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-4142f7342070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"OBAMA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Barack Obama\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Barack Obama lifts America one last time in emotional farewell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mspacy/matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.PhraseMatcher.__init__ (spacy/matcher.cpp:11371)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes at least 2 positional arguments (1 given)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"OBAMA\", [nlp(\"Barack Obama\")])\n",
    "doc = nlp(\"Barack Obama lifts America one last time in emotional farewell\")\n",
    "matches = matcher(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Resume Phrase Matcher code\n",
    "\n",
    "\n",
    "#importing all required libraries\n",
    "\n",
    "import PyPDF2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "#Function to read resumes from the folder one by one\n",
    "mypath='D:/NLP_Resume/Candidate Resume' #enter your path here where you saved the resumes\n",
    "onlyfiles = [os.path.join(mypath, f) for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n",
    "\n",
    "def pdfextract(file):\n",
    "    fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
    "    countpage = fileReader.getNumPages()\n",
    "    count = 0\n",
    "    text = []\n",
    "    while count < countpage:    \n",
    "        pageObj = fileReader.getPage(count)\n",
    "        count +=1\n",
    "        t = pageObj.extractText()\n",
    "        print (t)\n",
    "        text.append(t)\n",
    "    return text\n",
    "\n",
    "#function to read resume ends\n",
    "\n",
    "\n",
    "#function that does phrase matching and builds a candidate profile\n",
    "def create_profile(file):\n",
    "    text = pdfextract(file) \n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.lower()\n",
    "    #below is the csv where we have all the keywords, you can customize your own\n",
    "    keyword_dict = pd.read_csv('/home/mikes/Documents/ML/NLP/template.csv')\n",
    "    stats_words = [nlp(text) for text in keyword_dict['Statistics'].dropna(axis = 0)]\n",
    "    NLP_words = [nlp(text) for text in keyword_dict['NLP'].dropna(axis = 0)]\n",
    "    ML_words = [nlp(text) for text in keyword_dict['Machine Learning'].dropna(axis = 0)]\n",
    "    DL_words = [nlp(text) for text in keyword_dict['Deep Learning'].dropna(axis = 0)]\n",
    "    R_words = [nlp(text) for text in keyword_dict['R Language'].dropna(axis = 0)]\n",
    "    python_words = [nlp(text) for text in keyword_dict['Python Language'].dropna(axis = 0)]\n",
    "    Data_Engineering_words = [nlp(text) for text in keyword_dict['Data Engineering'].dropna(axis = 0)]\n",
    "\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add('Stats', None, *stats_words)\n",
    "    matcher.add('NLP', None, *NLP_words)\n",
    "    matcher.add('ML', None, *ML_words)\n",
    "    matcher.add('DL', None, *DL_words)\n",
    "    matcher.add('R', None, *R_words)\n",
    "    matcher.add('Python', None, *python_words)\n",
    "    matcher.add('DE', None, *Data_Engineering_words)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    d = []  \n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "        span = doc[start : end]  # get the matched slice of the doc\n",
    "        d.append((rule_id, span.text))      \n",
    "    keywords = \"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items())\n",
    "    \n",
    "    ## convertimg string of keywords to dataframe\n",
    "    df = pd.read_csv(StringIO(keywords),names = ['Keywords_List'])\n",
    "    df1 = pd.DataFrame(df.Keywords_List.str.split(' ',1).tolist(),columns = ['Subject','Keyword'])\n",
    "    df2 = pd.DataFrame(df1.Keyword.str.split('(',1).tolist(),columns = ['Keyword', 'Count'])\n",
    "    df3 = pd.concat([df1['Subject'],df2['Keyword'], df2['Count']], axis =1) \n",
    "    df3['Count'] = df3['Count'].apply(lambda x: x.rstrip(\")\"))\n",
    "    \n",
    "    base = os.path.basename(file)\n",
    "    filename = os.path.splitext(base)[0]\n",
    "       \n",
    "    name = filename.split('_')\n",
    "    name2 = name[0]\n",
    "    name2 = name2.lower()\n",
    "    ## converting str to dataframe\n",
    "    name3 = pd.read_csv(StringIO(name2),names = ['Candidate Name'])\n",
    "    \n",
    "    dataf = pd.concat([name3['Candidate Name'], df3['Subject'], df3['Keyword'], df3['Count']], axis = 1)\n",
    "    dataf['Candidate Name'].fillna(dataf['Candidate Name'].iloc[0], inplace = True)\n",
    "\n",
    "    return(dataf)\n",
    "        \n",
    "#function ends\n",
    "        \n",
    "#code to execute/call the above functions\n",
    "\n",
    "final_database=pd.DataFrame()\n",
    "i = 0 \n",
    "while i < len(onlyfiles):\n",
    "    file = onlyfiles[i]\n",
    "    dat = create_profile(file)\n",
    "    final_database = final_database.append(dat)\n",
    "    i +=1\n",
    "    print(final_database)\n",
    "\n",
    "    \n",
    "#code to count words under each category and visulaize it through Matplotlib\n",
    "\n",
    "final_database2 = final_database['Keyword'].groupby([final_database['Candidate Name'], final_database['Subject']]).count().unstack()\n",
    "final_database2.reset_index(inplace = True)\n",
    "final_database2.fillna(0,inplace=True)\n",
    "new_data = final_database2.iloc[:,1:]\n",
    "new_data.index = final_database2['Candidate Name']\n",
    "#execute the below line if you want to see the candidate profile in a csv format\n",
    "#sample2=new_data.to_csv('sample.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "ax = new_data.plot.barh(title=\"Resume keywords by category\", legend=False, figsize=(25,7), stacked=True)\n",
    "labels = []\n",
    "for j in new_data.columns:\n",
    "    for i in new_data.index:\n",
    "        label = str(j)+\": \" + str(new_data.loc[i][j])\n",
    "        labels.append(label)\n",
    "patches = ax.patches\n",
    "for label, rect in zip(labels, patches):\n",
    "    width = rect.get_width()\n",
    "    if width > 0:\n",
    "        x = rect.get_x()\n",
    "        y = rect.get_y()\n",
    "        height = rect.get_height()\n",
    "        ax.text(x + width/2., y + height/2., label, ha='center', va='center')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
